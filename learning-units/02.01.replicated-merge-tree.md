# ReplicatedMergeTree

> ğŸ’¡ Instructions about how to use this project can be found [here](../README.md).

Replication is the way ClickHouse solves high availability.  
This means that a shard (a portion of the data in a table) can be replicated in one or more nodes.

I know we mentioned shards but for the moment we will just pretent we didn't say it. Let's start by replicating entire tables.

At the moment of writing this, replication is at table level. This means you can have a database where only one or a subset of your tables are replicated across different nodes.

The nodes data can be replicated must be inside a `cluster`.

We went through `MergeTree`, `SummingMergeTree` and `AggregatingMergeTree`. Guess what prefix do we have to put in front of the engine name to make them replicated? You guessed correctly: in order to set replication to a `MergeTree` you write it as `ReplicatedMergeTree` and so on, in addition to some other settings that we will describe in a minute.

- [ReplicatedMergeTree](#replicatedmergetree)
  - [Database creation](#database-creation)
  - [Table creation](#table-creation)
    - [Checking cluster](#checking-cluster)
  - [Inserting data](#inserting-data)
  - [Wrapping up](#wrapping-up)

Let's create our first replicated MergeTree table.

## Database creation

Let's create a new database for this:

```sql
CREATE DATABASE IF NOT EXISTS replication ON CLUSTER cluster_1;
```

```sql
use replication;
```

What's all this? What's this cluster name?  
This project is being set to have a cluster of 3 nodes. As most (if not all) the information about your running ClickHouse instances, the details about the cluster are in the `system` database and, specifically, in the `clusters` table. Let's see:

```sql
SELECT
    cluster,
    shard_num,
    replica_num,
    host_address,
    is_local
FROM system.clusters
WHERE cluster = 'cluster_1'

Query id: bd9103cc-8a74-45d7-94b5-791aeb8b7948

â”Œâ”€clusterâ”€â”€â”€â”¬â”€shard_numâ”€â”¬â”€replica_numâ”€â”¬â”€host_addressâ”€â”¬â”€is_localâ”€â”
â”‚ cluster_1 â”‚         1 â”‚           1 â”‚ 172.22.0.5   â”‚        1 â”‚
â”‚ cluster_1 â”‚         1 â”‚           2 â”‚ 172.22.0.6   â”‚        0 â”‚
â”‚ cluster_1 â”‚         1 â”‚           3 â”‚ 172.22.0.4   â”‚        0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This means that there is a cluster called `cluster_1` where there are 3 nodes sharing the same and unique shard, their host addresses and that the one I made the query to (`is_local = 1`) is the replica 1.

## Table creation

Execute this:

```sql
CREATE TABLE IF NOT EXISTS basic ON CLUSTER cluster_1
(
    number UInt32
)
ENGINE = ReplicatedMergeTree('/clickhouse/tables/replication/basic/{shard}/', '{replica}')
ORDER BY (number);
```

Let's see what we can learn from this sentence.
We create a table (if it doesn't exist) on the cluster named `cluster_1`, to which this node must belong to (try to create it on another cluster name and it won't work!).  
The table engine is `ReplicatedMergeTree` which, oddly enough, means that it's a MergeTree table but also replicated, but there are also two so far unknown things, `'/clickhouse/tables/replication/basic/{shard}/'` and `'{replica}'`.

ClickHouse uses `Zookeeper` to orchestrate replication across nodes by storing in it meta data, without going much into details, `'/clickhouse/tables/replication/basic/{shard}/'` is the path in Zookeeper where this data is, which contains a `{shard}` variable that is interpolated from the `macros` definition.  

Think of `macros` as simple variables in a template engine that are defined in ClickHouse configuration files. Curious to see what they look like? Check this out:  [../docker/clickhouse/clickhouse_1.xml](../docker/clickhouse/clickhouse_1.xml).

If you have opened the file should know how `'{replica}'` gets its data from too. Each node in the cluster has its own `replica` number.

To start working with replicated tables, this is all you have to know, although it's a complex topic that requires careful planning when doing operations.

### Checking cluster

So far we have just created a database and a table. We can manually create the databases and tables in each node but it's much more convenient to use the `ON CLUSTER ...` modifier, as we did.  
Let's now see if it did the trick.

To do that we will connect to other nodes and check if they also have a `replication` database and a `basic` table.

Let's use docker-compose for that so we start exploring also the CLI.
In another terminal but in the same folder of the project type the following:

```bash
docker-compose exec clickhouse_2 bash -c "clickhouse-client -u default --password pass --query=\"show databases;\""
```

and

```bash
docker-compose exec clickhouse_3 bash -c "clickhouse-client -u default --password pass --query=\"show databases;\""
```

if the database `replication` showed up the databases are created correctly on all three nodes.

Let's now check the tables:

```bash
docker-compose exec clickhouse_2 bash -c "clickhouse-client -u default --password pass --query=\"SELECT name, engine FROM system.tables WHERE database = 'replication' FORMAT JSON;\""
```

```bash
docker-compose exec clickhouse_3 bash -c "clickhouse-client -u default --password pass --query=\"SELECT name, engine FROM system.tables WHERE database = 'replication' FORMAT JSON;\""
```

Great! All tables are there but I got the result as JSON? Yes, remember that your queries can format their output in many different ways! https://clickhouse.tech/docs/en/interfaces/formats/

You will find this extremely useful when doing queries that you want to immediately export to CSV, JSON, Parquet, Arrow, etc. from the command line.

Now let's see replication in action

## Inserting data

Back to our ClickHouse client:

```sql
INSERT INTO basic
SELECT 
    rand() as number
FROM numbers(10000000);
```

Data is being replicated as it's inserted.

Let's see if it's true or not. Again open a new terminal and let's check nodes 2 and 3.

```bash
docker-compose exec clickhouse_2 bash -c "clickhouse-client -u default --password pass --query=\"SELECT count() from replication.basic;\""
```

```bash
docker-compose exec clickhouse_3 bash -c "clickhouse-client -u default --password pass --query=\"SELECT count() from replication.basic;\""
```

## Wrapping up

Everything you have learned about `MergeTree`, `SummingMergeTree` and `AggregatingMergeTree` can be applied in a replicated context, becoming  `ReplicatedMergeTree`, `ReplicatedSummingMergeTree` and `ReplicatedAggregatingMergeTree` engines.

