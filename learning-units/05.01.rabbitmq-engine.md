# RabbitMQ Engine

> ðŸ’¡ Instructions about how to use this project can be found [here](../README.md).

We said ClickHouse favours big infrequent inserts rather than small frequent ones. Many times, however, this is not how our data is generated.  
There are different ways to solve this, of one which is to send data to ClickHouse through a message broker.  
Today we will use `RabbitMQ` to send data to ClickHouse. Luckily, if you started this project, you already have a running RabbitMQ node ready to be used.

https://clickhouse.tech/docs/en/engines/table-engines/integrations/rabbitmq/

**Important**: you can also use `Kafka` with for the same purpose.

## Our use case

We will simulate a use case where JSON messages are being sent to RabbitMQ and ClickHouse will fetch them by using a `RabbitMQ Engine`.  
The JSON string has the following format

```JSON
{
    "timestamp":"1626287099974805148",
    "id":86,
    "body":"my body is 86"
}
```

This is OK but there are some problems:

- We want to perform aggregations over this therefore we need to normalise the message and extract its parts to save them in different columns.
- the `timestamp` seems to be in nanoseconds but we want to save them at seconds precision.

Let's see how we can solve this.

- [RabbitMQ Engine](#rabbitmq-engine)
  - [Our use case](#our-use-case)
  - [Creating the schema](#creating-the-schema)
  - [Publish events](#publish-events)
  - [Wrapping up](#wrapping-up)

## Creating the schema

Before we create the schema, just know that ClickHouse requires its configuration XML to contain the user and password to connect to RabbitMQ. In this project you can find it [here](./../docker/clickhouse/config.xml)

Having said this, let's create the database we'll be playing with:

```sql
CREATE DATABASE IF NOT EXISTS event;
```

```sql
USE event;
```

```sql
CREATE TABLE IF NOT EXISTS event (
    `timestamp` DateTime,
    `id` UInt32,
    `body` String
) Engine = Memory();
```

(Now that we have learned how to use the `Memory()` engine we will use it quite often, since it's very handy for tests and demos.)  
This is the final table where our messages will be stored. As you see, it matches the JSON message structure.

```sql
CREATE TABLE IF NOT EXISTS rabbitmq_entry
(
    `timestamp` UInt64,
    `id` UInt32,
    `body` String
) ENGINE = RabbitMQ SETTINGS
    rabbitmq_host_port = 'rabbitmq:5672',
    rabbitmq_exchange_name = 'clickhouse-exchange',
    rabbitmq_routing_key_list = 'myqueue',
    rabbitmq_format = 'JSONEachRow',
    rabbitmq_exchange_type = 'fanout',
    rabbitmq_num_consumers = 1,
    rabbitmq_routing_key_list = 'myqueue'
;
```

This is the table implements the `RabbitMQ engine`, which connects to `RabbitMQ` and pulls messages to it, doesn't store it, but has it available for any `materialised view` that will read from it.
Conveniently, the `rabbitmq_format = 'JSONEachRow'` option parses messages sent in JSON format to be directly queried by any `materialised view`.

```sql
CREATE MATERIALIZED VIEW IF NOT EXISTS event_view
TO event AS
SELECT
    toDateTime(toUInt64(divide(timestamp, 1000000000))) AS timestamp,
    id AS id,
    body AS body
FROM rabbitmq_entry;
```

Finally, this is the materialised view that will, not only read any message received by the `rabbitmq_entry` table, but will also perform the required transformations to store the data as expected.  
We can see how the `toDateTime(toUInt64(divide(timestamp, 1000000000)))` expression will firstly transform the timestamp to a seconds precision epoch timestamp and then transform it to a `DateTime` type ready to be stored.

This is a common pattern. Materialised views are not only used to aggregate data but also to perform transformations to the data before they are stored.

## Publish events

Let's publish some messages with this one-liner:

```bash
docker-compose exec rabbitmq bash -c 'for i in {1..100}; do TIMESTAMP=$(($(date +%s%N))); echo "{\"timestamp\":\"$TIMESTAMP\",\"id\":$i,\"body\":\"my body is $i\"}" | rabbitmqadmin --username=admin --password=admin publish exchange=clickhouse-exchange routing_key=myqueue & done'
```

Now, let's back to ClickHouse and check if messages published to the broker have been fetched by ClickHouse.

```sql
SELECT * FROM event LIMIT 10;
```

Once data is in ClickHouse, it's business as usual.

## Wrapping up

Let's clean up a bit:

```sql
DROP DATABASE event SYNC;
```
