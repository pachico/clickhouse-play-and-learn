# MergeTree Materialised Views with AggregatingMergeTree

> ðŸ’¡ Instructions about how to use this project can be found [here](../README.md).

https://clickhouse.tech/docs/en/sql-reference/statements/create/view/#materialized

We learned how `SummingMergeTree` works, where the key word is `summing` but this is just one of the aggregations that you might need to do over your data.  
What about number of unique values? Or quantiles? Or the standard deviation or its simple mean average?  
All this can be done with another engine called `AggregatingMergeTree`.

Once again, we will have a source table where data in inserted, a materialised view and its target.

- [MergeTree Materialised Views with AggregatingMergeTree](#mergetree-materialised-views-with-aggregatingmergetree)
  - [Table creation](#table-creation)
  - [Inserting data](#inserting-data)
  - [Checking materialised data](#checking-materialised-data)
  - [Wrapping up](#wrapping-up)

## Table creation

```sql
CREATE DATABASE IF NOT EXISTS mergetree;
```

and use it (this means selecting it):

```sql
use mergetree;
```

and create the table we will insert data into:

```sql
CREATE TABLE IF NOT EXISTS source
(
    `timestamp` DateTime,
    `number` UInt32
)
ENGINE = MergeTree
PARTITION BY tuple()
ORDER BY timestamp;
```

Now we will create the table where the aggregate data will be stored and the logic that puts it there:

```sql
CREATE TABLE IF NOT EXISTS target
(
  `minute` DateTime,
  `count` SimpleAggregateFunction(sum, UInt64),
  `quantiles` AggregateFunction(quantile, UInt32),
  `uniqueness` AggregateFunction(uniq, UInt32)

) 
ENGINE =  AggregatingMergeTree()
ORDER BY minute;
```

Wow, lots of new things in here! What are these `SimpleAggregateFunction` and `AggregateFunction`? Those, my friend, are one of the unique features in ClickHouse.

We said ClickHouse can do real-time aggregation, which require some under-the-hood magic.
Some functions are quite easy to handle, like `sum`, `max` or `min` since they don't require to record the whole dataset. For instance, to perform a `sum` you simply store the results over sum operations. Or to store the `max` you compare each number with the so-far-max-number and, if a new number is bigger, you replace your current biggest with the new one.  
But other operations require to keep more data. Let's think of `avg` (mean average). In order to keep track of what's the average while more data is inserted, you need to keep the so-far-average plus the amount of items you have recorded.  
And some other operations require even more data, like quantiles, for instance.

So, this data is called `state` and is kept in a column in a format that you can query but using a special syntax that we will discover in a bit.

Now, a quick recap from the docs:

> Aggregate functions are stateful functions. They accumulate passed values into some state and allow you to get results from that state. They are managed with the IAggregateFunction interface. States can be rather simple (the state for AggregateFunctionCount is just a single UInt64 value) or quite complex (the state of AggregateFunctionUniqCombined is a combination of a linear array, a hash table, and a HyperLogLog probabilistic data structure).

https://clickhouse.tech/docs/en/development/architecture/#aggregate-functions

while 

> SimpleAggregateFunction(name, types_of_argumentsâ€¦) data type stores current value of the aggregate function, and does not store its full state as AggregateFunction does. This optimization can be applied to functions for which the following property holds: the result of applying a function f to a row set S1 UNION ALL S2 can be obtained by applying f to parts of the row set separately, and then again applying f to the results: f(S1 UNION ALL S2) = f(f(S1) UNION ALL f(S2)). This property guarantees that partial aggregation results are enough to compute the combined one, so we do not have to store and process any extra data.

https://clickhouse.tech/docs/en/sql-reference/data-types/simpleaggregatefunction/

https://clickhouse.tech/docs/en/development/architecture/#aggregate-functions

**Tip**: To see all the aggregate functions visit the page https://clickhouse.tech/docs/en/sql-reference/aggregate-functions/reference/.

```sql
CREATE MATERIALIZED VIEW IF NOT EXISTS target_view TO target AS
SELECT toStartOfMinute(timestamp) AS minute,
    count() AS count,
    quantileState(number) AS quantiles,
    uniqState(number) AS uniqueness
FROM source
GROUP BY minute
```

At this point, when doing `show tables` you should see 3 elements:

```sql
SHOW TABLES

â”Œâ”€nameâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ source      â”‚
â”‚ target      â”‚
â”‚ target_view â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3 rows in set. Elapsed: 0.002 sec. 
```

## Inserting data

```sql
INSERT INTO source
SELECT 
    now() - (rand() % 360) AS timestamp, 
    number AS number
FROM numbers(10000000)
```

## Checking materialised data

We just inserted 10 million rows and we will now check how to query the materialised view compared to the source table. Remember that you can do this while data is being constantly inserted. This is the amazing real-time aggregation! ;)

Querying the source table:

```sql
SELECT
    toStartOfMinute(timestamp) AS minute,
    count() AS count,
    round(quantile(0.9)(number)) AS quantiles,
    uniq(number) AS uniqueness
FROM source
GROUP BY minute
ORDER BY minute DESC

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€minuteâ”€â”¬â”€â”€â”€countâ”€â”¬â”€quantilesâ”€â”¬â”€uniquenessâ”€â”
â”‚ 2021-07-12 18:45:00 â”‚ 1111740 â”‚   9026133 â”‚    1112855 â”‚
â”‚ 2021-07-12 18:44:00 â”‚ 1665156 â”‚   8971302 â”‚    1664951 â”‚
â”‚ 2021-07-12 18:43:00 â”‚ 1667998 â”‚   9036914 â”‚    1667087 â”‚
â”‚ 2021-07-12 18:42:00 â”‚ 1668010 â”‚   8946422 â”‚    1660259 â”‚
â”‚ 2021-07-12 18:41:00 â”‚ 1665776 â”‚   8963889 â”‚    1674043 â”‚
â”‚ 2021-07-12 18:40:00 â”‚ 1665840 â”‚   8984101 â”‚    1649141 â”‚
â”‚ 2021-07-12 18:39:00 â”‚  555480 â”‚   8996260 â”‚     557562 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

7 rows in set. Elapsed: 0.174 sec. Processed 10.00 million rows, 80.00 MB (57.54 million rows/s., 460.32 MB/s.) 
```

and querying the materialised view:

```sql
SELECT
    minute,
    sum(count) AS count,
    round(quantileMerge(0.9)(quantiles)) AS quantiles,
    uniqMerge(uniqueness) AS uniqueness
FROM target
GROUP BY minute
ORDER BY minute DESC

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€minuteâ”€â”¬â”€â”€â”€countâ”€â”¬â”€quantilesâ”€â”¬â”€uniquenessâ”€â”
â”‚ 2021-07-12 18:45:00 â”‚ 1111740 â”‚   9014551 â”‚    1112855 â”‚
â”‚ 2021-07-12 18:44:00 â”‚ 1665156 â”‚   9012223 â”‚    1664951 â”‚
â”‚ 2021-07-12 18:43:00 â”‚ 1667998 â”‚   8984821 â”‚    1667087 â”‚
â”‚ 2021-07-12 18:42:00 â”‚ 1668010 â”‚   8976018 â”‚    1660259 â”‚
â”‚ 2021-07-12 18:41:00 â”‚ 1665776 â”‚   8960865 â”‚    1674043 â”‚
â”‚ 2021-07-12 18:40:00 â”‚ 1665840 â”‚   8966964 â”‚    1649141 â”‚
â”‚ 2021-07-12 18:39:00 â”‚  555480 â”‚   9009958 â”‚     557562 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

7 rows in set. Elapsed: 0.037 sec. 
```

Once again, great improvement: `0.174 sec` vs `0.037 sec`! But wait, quantiles don't match! "This is just not working!", you might say.

Good catch! Let's see what the docs say: https://clickhouse.tech/docs/en/sql-reference/aggregate-functions/reference/quantile/

> Computes an **approximate quantile** of a numeric data sequence.  
This function applies reservoir **sampling with a reservoir size up to 8192** and a random number generator for sampling. The result is non-deterministic. To get an exact quantile, use the **quantileExact** function.  

FYI, there's more about quantiles to learn: https://clickhouse.tech/docs/en/sql-reference/aggregate-functions/reference/quantiles/#quantiles

**Bonus tip**: since `quantileState` will store a reservoir sampling, it can be used for many quantile calculations with no extra storage cost.

Let's try it out:

```sql
SELECT
    minute,
    round(quantileMerge(0.9)(quantiles)) AS p90,
    round(quantileMerge(0.75)(quantiles)) AS p75,
    round(quantileMerge(0.5)(quantiles)) AS p50
FROM target
GROUP BY minute
ORDER BY minute DESC;
```

## Wrapping up

We will learn more advanced aggregation engines and techniches but for now imagine the amount of simple use cases you can solve with this technique!

Now, do some housekeeping:

```sql
DROP TABLE source SYNC;
DROP TABLE target SYNC;
DROP TABLE target_view SYNC;
```
